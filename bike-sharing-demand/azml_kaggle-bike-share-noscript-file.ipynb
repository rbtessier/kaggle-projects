{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Bike Share competition\n",
    "Background here is that I tried using this bike share script I had written to run linear regression predicting the counts of people taking out bikes from a bike share hourly. I did a bunch of data preprocessing based on what I was learning in PoDS, and then I tried using the Microsoft Azure ML tutorials to port this over to Azure ML. However, trying to be too ambitious, I changed too many things, and when I predict I get ridiculous values. I also coded it stupidly so that all my debugging couldn't be done with each run. \n",
    "\n",
    "I'm retrying it all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start by getting the Workspace in Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "#alternative Workspace.get (name=, subscription_id=, resource_group=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azureml.core.workspace.Workspace"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myCIWorkstationRyan : ComputeInstance\n",
      "myCClusterRyan : AmlCompute\n"
     ]
    }
   ],
   "source": [
    "for compute_name in ws.compute_targets:\n",
    "    compute = ws.compute_targets[compute_name]\n",
    "    print(compute.name , ':', compute.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I start with defining a function that\n",
    "1. Converts the `datetime` feature into a nominal \"time of day\" feature, then converts that into dummy variables\n",
    "2. Converts nominal feature `season` into dummy variables\n",
    "3. Converts nominal feature `weather` into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def when_is_it(hour):\n",
    "    if hour >=5 and hour < 11:\n",
    "        return 'morning'\n",
    "    elif hour >=11 and hour < 16:\n",
    "        return 'afternoon'\n",
    "    elif hour >=16 and hour < 18:\n",
    "        return 'rush_hour'\n",
    "    else:\n",
    "        return 'off_hours'\n",
    "\n",
    "def season_is_it(season_int):\n",
    "    if season_int == 1:\n",
    "        return 'spring'\n",
    "    elif season_int == 2:\n",
    "        return 'summer'\n",
    "    elif season_int == 3:\n",
    "        return 'fall'\n",
    "    else:\n",
    "        return 'winter'\n",
    "    \n",
    "def weather_is_it(weather_int):\n",
    "    if weather_int == 1:\n",
    "        return 'nice'\n",
    "    elif weather_int == 2:\n",
    "        return 'misty'\n",
    "    elif weather_int == 3:\n",
    "        return 'ugly'\n",
    "    else:\n",
    "        return 'stormy'\n",
    "    \n",
    "#function takes all nominal features and converts to dummy features\n",
    "def dummy_conversion(data): \n",
    "    #create the rules for each dummy variable\n",
    "    df_when_is_it = data['when_is_it'].apply(when_is_it)\n",
    "    df_season_is_it = data['season'].apply(season_is_it)\n",
    "    df_weather_is_it = data['weather'].apply(weather_is_it)\n",
    "    \n",
    "    \n",
    "    when_dummies = pd.get_dummies(df_when_is_it, prefix = 'when_')    \n",
    "    season_dummies = pd.get_dummies(df_season_is_it, prefix = 'season_')\n",
    "    weather_dummies = pd.get_dummies(df_weather_is_it, prefix = 'weather_')\n",
    "    \n",
    "    #drop the old nominal veriables\n",
    "    data=data.drop('datetime', axis = 1 )\n",
    "    data=data.drop('season', axis = 1)\n",
    "    data=data.drop('weather', axis = 1)\n",
    "    \n",
    "    data[list(when_dummies.columns)] = when_dummies\n",
    "    data[list(season_dummies.columns)] = season_dummies\n",
    "    data[list(weather_dummies.columns)] = weather_dummies\n",
    "    return data                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Experiment\n",
    " remember toimport Experiment from `azureml.core`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(workspace=ws, name='bike-sharing-demand-LinRegBest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load data and prepare for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_csv('train.csv')\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "temp_datetime = pd.read_csv('test.csv')['datetime']\n",
    "\n",
    "feature_cols = ['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
    "       'atemp', 'humidity', 'windspeed']    \n",
    "\n",
    "y = bikes['count']\n",
    "bikes=bikes[feature_cols]\n",
    "\n",
    "bikes['when_is_it']= bikes['datetime'].apply(lambda x:int(x[11]+x[12]))\n",
    "test_data['when_is_it'] = test_data['datetime'].apply(lambda x:int(x[11]+x[12]))\n",
    "                      \n",
    "\n",
    "\n",
    "\n",
    "X=dummy_conversion(bikes)\n",
    "test_data=dummy_conversion(test_data)\n",
    "\n",
    "#Apply scaler to the features - I was told this helps with many algorithms - it seemed to help out here\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    #X_train[feature_cols] = scaler.transform(X_train[feature_cols])\n",
    "    #X_test[feature_cols] = scaler.transform(X_test[feature_cols])\n",
    "    #X_train and y_train will be used to train the model\n",
    "    #X_test and y_test will be used to test the model\n",
    "    #remember that all four of these variables are just subsets of the overall X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train a model\n",
    "\n",
    "Following design pattern shows how to leverage the SDK to easily keep track of your training in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE is 1.177293425470474\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "#create the run\n",
    "#tutorial usese this to start the experiment, run.get_context() is used in MS Learn\n",
    "run = experiment.start_logging()\n",
    "\n",
    "model = LinearRegression()\n",
    "    #instantiate the model\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "    # fit the model to our training set\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "    #predict our testing set\n",
    "    #y_test = y_test.abs()\n",
    "    \n",
    "    #the log error doesn't take negative values, linear regression however can predict negative counts.\n",
    "    #since this makes no sense, I make any negative predictions = 0.\n",
    "y_pred[y_pred <0] = 0\n",
    "    \n",
    "error = np.sqrt(metrics.mean_squared_log_error(y_test, y_pred))\n",
    "run.log('RMSLE', error)\n",
    "    #error = np.sqrt(metrics.mean_squared_log_error(y_test, y_pred))\n",
    "    # calculate our metric\n",
    "    \n",
    "print('RMSLE is ' + str(error))\n",
    "\n",
    "run.complete()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(test_data)\n",
    "test_data=scaler.transform(test_data)\n",
    "\n",
    "predictions = linreg.predict(test_data)\n",
    "#ensure all negative values predict 0\n",
    "predictions[predictions <0] = 0 \n",
    "\n",
    "#ensure all counts are rounded\n",
    "predictions = np.rint(predictions)\n",
    "submission = pd.DataFrame({'datetime': temp_datetime, 'count': predictions})\n",
    "submission.to_csv('linReg_submission2', index=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`11:06 Aug 1: ` So here's a fun observation, when I was doing up this notebook, I had the code separated out in such a way that I would run the code above and the RMSLE wouldn't change. And I was like... What? I examined above, and the X and Y split happened earlier, and it appears the RMSLE is varying due to the randomness in the training/test split.\n",
    "\n",
    "This inspired me to go back above and change up my code to test out 18 (20-1 splits), here I will show it below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 0.05RMSLE is 1.073977492798418\n",
      "test size: 0.1RMSLE is 1.109083093933371\n",
      "test size: 0.15RMSLE is 1.1194627874171397\n",
      "test size: 0.2RMSLE is 1.1626191128813523\n",
      "test size: 0.25RMSLE is 1.1373476297400904\n",
      "test size: 0.3RMSLE is 1.137103137328353\n",
      "test size: 0.35RMSLE is 1.138879107572037\n",
      "test size: 0.4RMSLE is 1.1551912703890983\n",
      "test size: 0.45RMSLE is 1.1713541647048589\n",
      "test size: 0.5RMSLE is 1.1575844389352992\n",
      "best (split, RMSLE)\n",
      "0.05 1.073977492798418\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "#create the run\n",
    "\n",
    "test_splits = [0.05, 0.1, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.5]\n",
    "\n",
    "best_split = None\n",
    "best_split_error=None\n",
    "#we are running an experiment where we are testing different test sizes, since train_test_split randomized, we test 20 different splits (arbitrarily chosen) to find the lowest \n",
    "#RMSLE, once we find the lowest among the 20 we log it and \n",
    "for split in test_splits:\n",
    "    run = experiment.start_logging()\n",
    "    run.log('test-size: ', split)\n",
    "    model = LinearRegression()\n",
    "    min_error=None\n",
    "    \n",
    "    for i in range(20):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split)\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        # fit the model to our training set\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        #predict our testing set\n",
    "        #y_test = y_test.abs()\n",
    "    \n",
    "        #the log error doesn't take negative values, linear regression however can predict negative counts.\n",
    "        #since this makes no sense, I make any negative predictions = 0.\n",
    "        y_pred[y_pred <0] = 0\n",
    "        \n",
    "        \n",
    "        current_error = np.sqrt(metrics.mean_squared_log_error(y_test, y_pred))\n",
    "        #checks if there's a min error, if there is, set to lowest of current and previous iteration\n",
    "        if min_error==None:\n",
    "            min_error = current_error\n",
    "        else:\n",
    "            if min_error > current_error:\n",
    "                min_error=current_error\n",
    "    run.log('RMSLE', min_error)\n",
    "    \n",
    "    if best_split_error== None:\n",
    "        best_split_error=min_error\n",
    "        best_split = split\n",
    "    else:\n",
    "        if best_split_error > min_error:\n",
    "            best_split_error = min_error\n",
    "            best_split = split\n",
    "        #error = np.sqrt(metrics.mean_squared_log_error(y_test, y_pred))\n",
    "        # calculate our metric\n",
    "    print('test size: ' + str(split) + 'RMSLE is ' + str(min_error))\n",
    "    run.complete()\n",
    "    \n",
    "    model_name = \"model_split_\" + str(split) + \".pkl\"\n",
    "    filename = \"outputs/\" + model_name\n",
    "\n",
    "    joblib.dump(value=model, filename=filename)\n",
    "    run.upload_file(name=model_name, path_or_stream=filename)\n",
    "    \n",
    "print('best (split, RMSLE)')\n",
    "print(best_split, best_split_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_split_0.1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3a7b16307769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_splits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_split_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#ensure all negative values predict 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ryan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_split_0.1.pkl'"
     ]
    }
   ],
   "source": [
    "#loads the saved model\n",
    "#fits test data to scaler\n",
    "scaler.fit(test_data)\n",
    "test_data=scaler.transform(test_data)\n",
    "\n",
    "for split in test_splits:\n",
    "    model = joblib.load('model_split_' + str(split) + '.pkl')\n",
    "    predictions = model.predict(test_data)\n",
    "    #ensure all negative values predict 0\n",
    "    predictions[predictions <0] = 0 \n",
    "\n",
    "    #ensure all counts are rounded\n",
    "    predictions = np.rint(predictions)\n",
    "    submission = pd.DataFrame({'datetime': temp_datetime, 'count': predictions})\n",
    "    submission.to_csv('linReg_submission_'+ str(split) + '.csv', index=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After submitting, I actually got slightly worse than last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6ae42a278db1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"model_alpha_0.1.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_run' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/bike-sharing-demand\n",
    "Above is the link I submit to. I officially got 1.46 RMSLE on the site when I submitted without dummy variables, but included scaling. \n",
    "\n",
    "My new submission... GOT 1.199984 = 1.2 RMSLE WOOOOOOOOO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
